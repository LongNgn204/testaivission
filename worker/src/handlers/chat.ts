/**
 * ============================================================
 * üí¨ Chat Handler - v3 (Context + Safety + No disclaimers)
 * ============================================================
 */

import { IRequest } from 'itty-router'
import { generateWithCloudflareAI } from '../services/gemini'
import { runTwoPassGptOss } from '../services/gptOss'
import { getChatContext, appendChatContext, renderContextAsText } from '../services/chatContext'
import { evaluateContentSafety } from '../services/contentSafety'
import { isBreakerOpen, recordFailure, recordSuccess } from '../services/circuitBreaker'
import { verifyAuthToken } from './auth'

const getSystemPrompt = (language: 'vi' | 'en') => {
  return language === 'vi'
    ? `B·∫°n l√† TI·∫æN Sƒ® - B√ÅC Sƒ® EVA, b√°c sƒ© nh√£n khoa l√¢m s√†ng.

M·ª•c ti√™u: h·ªó tr·ª£ ng∆∞·ªùi d√πng theo phong c√°ch "b√°c sƒ© chu·∫©n" ‚Äî h·ªèi ƒë√∫ng tr·ªçng t√¢m, suy lu·∫≠n t·ª´ b·ªëi c·∫£nh, linh ho·∫°t thay v√¨ khu√¥n m·∫´u.

Ki·∫øn th·ª©c tr·ªçng t√¢m: t·∫≠t kh√∫c x·∫°, b·ªánh ho√†ng ƒëi·ªÉm/v√µng m·∫°c, kh√¥ m·∫Øt & h·ªôi ch·ª©ng th·ªã gi√°c m√†n h√¨nh, dinh d∆∞·ª°ng m·∫Øt, ph·∫´u thu·∫≠t kh√∫c x·∫°, c·∫•p c·ª©u m·∫Øt c∆° b·∫£n.

C√°ch giao ti·∫øp:
- Ng·∫Øn g·ªçn, t·ª± nhi√™n, ·∫•m √°p, chuy√™n nghi·ªáp; 100% ti·∫øng Vi·ªát.
- ƒê·ªô d√†i m·∫∑c ƒë·ªãnh: 2‚Äì6 c√¢u; ch·ªâ m·ªü r·ªông chi ti·∫øt khi ng∆∞·ªùi d√πng y√™u c·∫ßu (v√≠ d·ª•: "gi·∫£i th√≠ch k·ªπ h∆°n", "v√¨ sao").
- Tr·∫£ l·ªùi-ngay: m·ªü ƒë·∫ßu b·∫±ng c√¢u tr·∫£ l·ªùi tr·ª±c ti·∫øp, ng·∫Øn g·ªçn d·ª±a tr√™n d·ªØ li·ªáu hi·ªán c√≥; ch·ªâ h·ªèi th√™m t·ªëi ƒëa 2‚Äì4 c√¢u n·∫øu th·∫≠t s·ª± c·∫ßn.
- B·∫Øt ƒë·∫ßu b·∫±ng 1‚Äì2 c√¢u ch√†o/ng·∫Øn x√°c nh·∫≠n m·ª•c ti√™u.
- Khi th√¥ng tin ch∆∞a ƒë·ªß: ∆∞u ti√™n H·ªéI 2‚Äì4 c√¢u h·ªèi ch·ªçn l·ªçc (tri·ªáu ch·ª©ng ch√≠nh, th·ªùi ƒëi·ªÉm kh·ªüi ph√°t, m·∫Øt n√†o, m·ª©c ƒë·ªô/ti·∫øn tri·ªÉn, ƒë·ªè/ƒëau/ch√≥i/ti·∫øt d·ªãch/ru·ªìi bay-ch·ªõp s√°ng, ti·ªÅn s·ª≠ m·∫Øt & b·ªánh to√†n th√¢n/thu·ªëc, ch·∫•n th∆∞∆°ng, k√≠nh √°p tr√≤ng/th·ªùi gian m√†n h√¨nh, thai k·ª≥, c·∫≠n n·∫∑ng & ti·ªÅn s·ª≠ gia ƒë√¨nh).
- N·∫øu h·ªØu √≠ch m·ªõi d√πng nh√£n ƒê√°nh gi√°/Khuy·∫øn ngh·ªã/Ti√™n l∆∞·ª£ng/M·ª©c ƒë·ªô kh·∫©n c·∫•p; KH√îNG b·∫Øt bu·ªôc. Tr√°nh m√°y m√≥c.

T√≠ch h·ª£p d·ªØ li·ªáu:
- N·∫øu c√≥ "K·∫øt qu·∫£ test g·∫ßn nh·∫•t", h√£y t√≥m t·∫Øt ng·∫Øn g·ªçn (kh√¥ng ch√©p nguy√™n vƒÉn) v√† l·ªìng gh√©p v√†o ƒê√°nh gi√°/Ph√¢n t√≠ch. Kh√¥ng b·ªãa ƒë·∫∑t x√©t nghi·ªám/ch·∫©n ƒëo√°n. Kh√¥ng n√≥i v·ªÅ "b√°o c√°o AI kh√¥ng th·ªÉ t·∫°o" tr·ª´ khi ng∆∞·ªùi d√πng n√™u r√µ.
- Heuristic l√¢m s√†ng: b·∫•t th∆∞·ªùng Amsler ‚Üí ∆∞u ti√™n v·∫•n ƒë·ªÅ HO√ÄNG ƒêI·ªÇM (tho√°i h√≥a ho√†ng ƒëi·ªÉm, ph√π ho√†ng ƒëi·ªÉm, m√†ng tr∆∞·ªõc v√µng m·∫°c), kh√¥ng quy cho ƒë·ª•c th·ªßy tinh th·ªÉ.

Ph√¢n t·∫ßng kh·∫©n c·∫•p (nh·ªõ gi·∫£i th√≠ch l√Ω do):
- üü¢ T·ª± theo d√µi
- üü° Kh√°m s·ªõm (72h‚Äì7 ng√†y)
- üî¥ Kh√°m trong 24‚Äì48h
- üü£ C·∫•p c·ª©u ngay (m·∫•t th·ªã l·ª±c ƒë·ªôt ng·ªôt, ƒëau m·∫Øt d·ªØ d·ªôi, ch·∫•n th∆∞∆°ng xuy√™n, h√≥a ch·∫•t, m√†n s∆∞∆°ng k√®m ƒëau/ƒë·ªè, ru·ªìi bay-ch·ªõp s√°ng m·ªõi k√®m r√®m che‚Ä¶)

An to√†n & t√≠nh ph√π h·ª£p:
- Ch·ªâ ·ªü L∆Ø·ª¢T ƒê·∫¶U TI√äN (khi kh√¥ng c√≥ l·ªãch s·ª≠ h·ªôi tho·∫°i ·ªü tr√™n), th√™m m·ªôt c√¢u nh·∫Øc ng·∫Øn: "Eva cung c·∫•p th√¥ng tin, kh√¥ng thay th·∫ø ch·∫©n ƒëo√°n c·ªßa b√°c sƒ©. N·∫øu c√≥ d·∫•u hi·ªáu c·∫•p c·ª©u, h√£y ƒëi c·∫•p c·ª©u ngay." Kh√¥ng l·∫∑p l·∫°i ·ªü c√°c l∆∞·ª£t sau. Kh√¥ng k√®m tuy√™n b·ªë ph√°p l√Ω d√†i.
- Khi ng∆∞·ªùi d√πng ch·ªâ g·ª≠i "hi/oke" ho·∫∑c t∆∞∆°ng t·ª±, tr·∫£ l·ªùi r·∫•t ng·∫Øn v√† ƒë·∫∑t c√¢u h·ªèi khai th√°c thay v√¨ in b√°o c√°o d√†i.

ƒê·ªãnh d·∫°ng:
- D√πng ti√™u ƒë·ªÅ ng·∫Øn ho·∫∑c bullet khi h·ªØu √≠ch; tr√°nh l·∫∑p khu√¥n.
- K·∫øt th√∫c b·∫±ng 1‚Äì3 c√¢u h·ªèi ∆∞u ti√™n n·∫øu c√≤n thi·∫øu d·ªØ li·ªáu, d∆∞·ªõi nh√£n "C·∫ßn th√™m:".`
    : `You are DR. EVA, a clinical ophthalmologist.

Goal: behave like a thoughtful clinician‚Äîask targeted questions, reason from context, be flexible (not templated).

Core knowledge: refractive errors, macula/retina, dry eye & computer vision syndrome, ocular nutrition, refractive surgery, basic eye emergencies.

Conversation style:
- Concise, natural, warm, professional; 100% English.
- Default length: 2‚Äì6 sentences; expand only when the user asks (e.g., "explain more", "why").
- Start with a brief greeting/goal check.
- If data is insufficient: ASK 2‚Äì4 focused questions (chief symptom, onset/timeline, which eye, severity/course, red/pain/photophobia/discharge/floaters-flashes, ocular/systemic history & meds, trauma, contact lens/screen time, pregnancy, high myopia & family history).
- Once sufficient: present a flexible structure (Assessment ‚Üí Analysis ‚Üí Recommendations ‚Üí Prognosis ‚Üí Urgency). Avoid rigidity.

Data integration:
- If a "lastTestResult" is provided, summarize it briefly (do not copy verbatim) and weave into Assessment/Analysis. Do not invent tests/diagnoses. Do not mention "AI report unavailable" unless the user says so.
- Clinical heuristic: Amsler abnormalities ‚Üí prioritize MACULAR causes (AMD, macular edema, epiretinal membrane), not cataract.

Triage (explain why):
- üü¢ Self-monitor
- üü° See within 72h‚Äì7 days
- üî¥ See within 24‚Äì48h
- üü£ Emergency now (sudden vision loss, severe eye pain, penetrating trauma/chemical injury, painful red hazy vision, new floaters-flashes with curtain, etc.)

Safety & appropriateness:
- Only on the FIRST TURN (when no prior conversation is shown above), add one short reminder: "Eva provides information and does not replace a doctor's diagnosis. For emergencies, seek urgent care immediately." Do not repeat. No long legal disclaimers.
- For minimal inputs like "hi/ok", keep it very short and ask clarifying questions instead of dumping a full report.

Formatting:
- Use short headers or bullets when helpful; avoid rigid templates.
- If more info is needed, end with 3‚Äì5 prioritized questions under "Need more:".`;
};

function sanitize(input: string): string {
  return (input || '')
    .replace(/<[^>]*>/g, '')
    .replace(/[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]/g, '')
    .trim()
    .slice(0, 1000)
}

async function resolveContextId(request: Request, env: any): Promise<{ id: string; source: 'user' | 'ip' }> {
  try {
    const auth = request.headers.get('authorization') || ''
    const token = auth.startsWith('Bearer ') ? auth.slice(7) : ''
    if (token && env.JWT_SECRET) {
      const decoded: any = await verifyAuthToken(token, env)
      if (decoded?.sub || decoded?.userId) {
        const uid = decoded.sub || decoded.userId
        return { id: `user:${uid}`, source: 'user' }
      }
    }
  } catch {}
  const ip = request.headers.get('cf-connecting-ip') || request.headers.get('x-forwarded-for') || 'anonymous'
  return { id: `ip:${ip}`, source: 'ip' }
}

export async function chat(request: IRequest, env: any): Promise<Response> {
  const req = request as unknown as Request
  try {
    const { message, lastTestResult, userProfile, language, model, temperature, topP, maxTokens } = (await req.json()) as any

    if (!message || !['vi','en'].includes(language)) {
      return new Response(JSON.stringify({ error: 'Bad request' }), { status: 400, headers: { 'Content-Type': 'application/json' } })
    }

    // Content safety
    const safety = evaluateContentSafety(String(message), language)
    if (!safety.allowed) {
      const safeMsg = safety.message || (language === 'vi' ? 'N·ªôi dung n√†y kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£.' : 'This content is not supported.')
      return new Response(JSON.stringify({ message: safeMsg, timestamp: new Date().toISOString(), language }), { status: 200, headers: { 'Content-Type': 'application/json' } })
    }

    if (!env.AI) {
      const msg = language === 'vi' ? 'D·ªãch v·ª• AI ch∆∞a s·∫µn s√†ng.' : 'AI service not available.'
      return new Response(JSON.stringify({ message: msg }), { status: 200, headers: { 'Content-Type': 'application/json' } })
    }

    if (await isBreakerOpen(env.CACHE)) {
      const msg = language === 'vi' ? 'H·ªá th·ªëng ƒëang b·∫≠n. Vui l√≤ng th·ª≠ l·∫°i sau √≠t ph√∫t.' : 'System is busy. Please try again shortly.'
      return new Response(JSON.stringify({ message: msg, timestamp: new Date().toISOString(), language }), { status: 200, headers: { 'Content-Type': 'application/json' } })
    }

    const identity = await resolveContextId(req, env)
    const ctxTurns = await getChatContext(env.CACHE, identity.id)
    const ctxText = renderContextAsText(ctxTurns, language)

    const sanitized = sanitize(String(message))
    let userPrompt = ''
    if (ctxText) userPrompt += `${ctxText}\n\n---\n`
    userPrompt += sanitized
    if (lastTestResult) userPrompt += `\n\nK·∫øt qu·∫£ test g·∫ßn nh·∫•t: ${JSON.stringify(lastTestResult)}`
    if (userProfile) userPrompt += language === 'vi'
      ? `\n\nH·ªì s∆° ng∆∞·ªùi d√πng: ${JSON.stringify(userProfile)}`
      : `\n\nUser profile: ${JSON.stringify(userProfile)}`

    const t0 = Date.now()
    let assistantCore = ''
    const requestId = crypto.randomUUID ? crypto.randomUUID() : `${Date.now()}-${Math.random()}`
    
    // Ch√∫ th√≠ch: log function ƒë·ªÉ track 2-pass
    const logJson = (event: string, meta?: Record<string, unknown>) => {
      console.log(JSON.stringify({ event, ...meta }))
    }

    try {
      const mdl = model || env.CHAT_MODEL || ''
      if (mdl.startsWith('gemini-')) {
        const { createGeminiFromEnv } = await import('../services/gemini')
        const gem = createGeminiFromEnv(env)
        assistantCore = await gem.generateContent(userPrompt, {
          model: mdl,
          temperature: typeof temperature === 'number' ? temperature : 0.7,
          maxTokens: typeof maxTokens === 'number' ? Math.max(64, Math.floor(maxTokens)) : 1200,
          topP: typeof topP === 'number' ? Math.min(Math.max(topP, 0), 1) : 0.8,
        })
      } else {
        // Ch√∫ th√≠ch: d√πng GPT-OSS-120B v·ªõi 2-pass accuracy check thay v√¨ llama
        const systemPrompt = getSystemPrompt(language)
        const { text } = await runTwoPassGptOss(env.AI, logJson, {
          requestId,
          instructions: systemPrompt,
          userInput: userPrompt,
        })
        assistantCore = text
      }
      await recordSuccess(env.CACHE)
    } catch (e) {
      await recordFailure(env.CACHE)
      const msg = language === 'vi' ? 'Xin l·ªói, AI ƒëang b·∫≠n. Vui l√≤ng th·ª≠ l·∫°i sau.' : 'Sorry, the AI is busy. Please try again later.'
      return new Response(JSON.stringify({ message: msg, timestamp: new Date().toISOString(), language }), { status: 200, headers: { 'Content-Type': 'application/json' } })
    }

    // Persist context (assistant message)
    try { await appendChatContext(env.CACHE, identity.id, sanitized, assistantCore) } catch {}

    // Cost & latency tracking (approximate tokens)
    try {
      const uid = identity.id.startsWith('user:') ? identity.id.slice(5) : null
      const tokensIn = Math.ceil((getSystemPrompt(language).length + userPrompt.length) / 4)
      const tokensOut = Math.ceil(assistantCore.length / 4)
      const latency = Date.now() - t0
      const { DatabaseService } = await import('../services/database')
      const db = new DatabaseService(env.DB)
      await db.trackCost({ userId: uid, service: 'llm', endpoint: '/api/chat', tokensInput: tokensIn, tokensOutput: tokensOut, costUsd: 0 })
      console.info(JSON.stringify({ evt: 'chat_done', model: model || env.CHAT_MODEL || '@cf/openai/gpt-oss-120b', tokensIn, tokensOut, latency }))
    } catch {}

    return new Response(
      JSON.stringify({ message: assistantCore, timestamp: new Date().toISOString(), language, model: model || env.CHAT_MODEL || '@cf/openai/gpt-oss-120b' }),
      { status: 200, headers: { 'Content-Type': 'application/json' } }
    )
  } catch (error: any) {
    return new Response(JSON.stringify({ error: 'Failed to process chat', message: error?.message || 'UNKNOWN' }), { status: 500, headers: { 'Content-Type': 'application/json' } })
  }
}
